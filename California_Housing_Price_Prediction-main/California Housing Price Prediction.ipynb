{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Data\n",
    "As we can see we first need to download the data from online sources. This can be done manually or by writing a small python script to download and extract the csv file from the tgz file. Writing a python script is preffered because data keeps changing from time to time also a python script can be automated so that new data is fetched from time to time so tha twe can keep a\n",
    "track of model's accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT=\"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH=os.path.join(\"datasets\",\"housing\")#creating a newfolder dataset and a housinh folder inside it\n",
    "HOUSING_URL=DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"#path where file will be stored\n",
    "\n",
    "#fuction to fetch data from online source\n",
    "def fetch_housing_data(housing_url=HOUSING_URL,housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):#used to check if a specified path in an existing directory or not\n",
    "        os.makedirs(housing_path)#used to create a directry recursively\n",
    "    tgz_path=os.path.join(housing_path,\"housing.tgz\")#will store tgz file inside housing path \n",
    "    urllib.request.urlretrieve(housing_url,tgz_path)#will store the downloaded file to workind directory\n",
    "    housing_tgz=tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()#calling the function\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path=os.path.join(housing_path,\"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing=load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above sequence total number of instances in the dataset are 20640. If we observe carefully we can also see total_bedrooms features has on 20433 non-null values which means some of the values are missing so we will have to on this problem. Also datatype of all features is same except ocean_proximity so we will check it out too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.ocean_proximity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50,figsize=(20,15))#bins is used for the width of histogram line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(data,test_ratio):\n",
    "    shuffled_indices=np.random.permutation(len(data))#randomly permute a sequence or return a permuted range\n",
    "    test_set_size=int(len(data)*test_ratio)\n",
    "    test_indices=shuffled_indices[:test_set_size]\n",
    "    train_indices=shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices],data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set=split_train_test(housing,0.2)\n",
    "len(train_set)\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.cut function is used to create a income category attribute with 5 categories\n",
    "housing[\"income_cat\"]=pd.cut(housing[\"median_income\"],\n",
    "                             bins=[0.,1.5,3.0,4.5,6.,np.inf],\n",
    "                            labels=[1,2,3,4,5])\n",
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index, test_index in split.split(housing,housing[\"income_cat\"]):\n",
    "    strat_train_set=housing.loc[train_index]\n",
    "    strat_test_set=housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts()/len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set,strat_test_set):\n",
    "    set_.drop(\"income_cat\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a copy of train set\n",
    "housing=strat_train_set.copy()\n",
    "housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above graph is hard to visualize so we will set alpha=0.1 to see more denser area clearly\n",
    "housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.5,\n",
    " s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    " c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ")\n",
    "plt.legend()\n",
    "# s represents districts population\n",
    "# c color represents the price\n",
    "# cmap we used predefined color map called jet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using correlation coefficient\n",
    "corr_matrix=housing.corr()\n",
    "corr_matrix.median_house_value.sort_values(ascending=False)\n",
    "#as we have to only see correlation of every feature with only median_house_value whic is our label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pandas scatter_matrix functi to find correlation\n",
    "#as there are 11 numerical features we will get 121 plots which is not fisible\n",
    "#so we will working with features that are reslly important\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes=[\"median_house_value\",\"median_income\",\"total_rooms\",\n",
    "           \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes],figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#median_house_value vs median_income is very important plot so lets take a closer look\n",
    "housing.plot(kind=\"scatter\",x=\"median_income\",y=\"median_house_value\",alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Attribute Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"]=housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"]=housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing=strat_train_set.drop(\"median_house_value\",axis=1)\n",
    "housing_labels=strat_train_set.median_house_value.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as we know total bedrooms had some non-null values so we must remove them before applying ML algo\n",
    "median=housing.total_bedrooms.median()\n",
    "housing.total_bedrooms.fillna(median,inplace=True)\n",
    "#housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scikit_learn provides a handy class to deal with missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as median only works in numerical values we will be removing feature ocean proximity\n",
    "housing_num=housing.drop(\"ocean_proximity\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(housing_num)\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use this trained imputer to transform the training setby replacing missing values by the learned medians\n",
    "X=imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a NumPy array containing transformed feature.\n",
    "# we can put it back in dataframe using following frame\n",
    "housing_tr=pd.DataFrame(X,columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Text and Categorial Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat=housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting these text to numbers\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_encoded=ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorial_ is used to get a list of categories\n",
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot =cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all you need is to create a class and implement three methods: fit()(returning self), transform(), and fit_transform(). You can get the last one forfree by simply adding TransformerMixin as a base class. Also, if you add BaseEstimator as a base class (and avoid *args and kargs in your constructor) you will gettwo extra methods (get_params() and set_params()) that will be useful for autoPrepare the Data for automatic hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix=3,4,5,6\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room=add_bedrooms_per_room\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household=X[:,rooms_ix]/X[:,household_ix]\n",
    "        population_per_household=X[:,population_ix]/X[:,household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]\n",
    "            return np.c_[X,rooms_per_household,population_per_household,\n",
    "                        bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X,rooms_per_household,population_per_household]\n",
    "    \n",
    "attr_adder=CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs=attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline=Pipeline([('imputer',SimpleImputer(strategy=\"median\")),\n",
    "                      ('attribs_adder',CombinedAttributesAdder()),\n",
    "                      ('std_scaler',StandardScaler())])\n",
    "housing_num_tr=num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.values[:,bedrooms_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribs=list(housing_num)\n",
    "cat_attribs=[\"ocean_proximity\"]\n",
    "full_pipeline=ColumnTransformer([\n",
    "    (\"num\",num_pipeline,num_attribs),\n",
    "    (\"cat\",OneHotEncoder(),cat_attribs)\n",
    "])\n",
    "housing_prepared=full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(housing_prepared,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data=housing.iloc[:5] # this takes data from 1st row to 4th row \n",
    "some_labels=housing_labels.iloc[:5]\n",
    "some_data_prepared=full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\",lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels:\",list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be measuring linear regression model's RMSE on the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predicions=lin_reg.predict(housing_prepared)\n",
    "lin_mse=mean_squared_error(housing_labels,housing_predicions)\n",
    "lin_rmse=np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not satisfactory as most districts median_housing values range\n",
    "between $120,000  and  $265,000 so a typical prediction error of $68,628 is not very satifactory. This is an example of model underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will train a more powerful model to solve the problem of overfitting\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg=DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions=tree_reg.predict(housing_prepared)\n",
    "tree_mse=mean_squared_error(housing_labels,housing_predictions)\n",
    "tree_rmse=np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the value of the rmse is 0 which means the model may badly overfit. So we will preform cross-validation feature to check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_score allows to randomly split the training set into 10 distinct susbse called folds\n",
    "# then evaluates the model 10 times\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores=cross_val_score(tree_reg,housing_prepared,housing_labels,\n",
    "                      scoring=\"neg_mean_squared_error\",cv=10)\n",
    "tree_rmse_scores=np.sqrt(-scores)#we hve used -scores bcoz cross_val_score function\n",
    "# except a utility func rather than cost function so it is opposite of MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_score(scores):\n",
    "    print(\"Scores:\",scores)\n",
    "    print(\"Mean:\",scores.mean())\n",
    "    print(\"Standard deviation:\",scores.std())\n",
    "display_score(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see after the cross validation the results of decison tree model are \n",
    "worse than the linear regressor. So the model was overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now we will try cross_validation on linear regressor model\n",
    "lin_scores=cross_val_score(lin_reg,housing_prepared,housing_labels,\n",
    "                          scoring=\"neg_mean_squared_error\",cv=10)\n",
    "lin_rmse_scores=np.sqrt(-lin_scores)\n",
    "display_score(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decison tree also does not work very well so we will now aaply Randaom forest Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg=RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions=forest_reg.predict(housing_prepared)\n",
    "forest_mse=mean_squared_error(housing_labels,housing_predictions)\n",
    "forest_rmse=np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_scores=cross_val_score(forest_reg,housing_prepared,housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\",cv=10)\n",
    "forest_rmse_score=np.sqrt(-forest_scores)\n",
    "display_score(forest_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid=[\n",
    "    {'n_estimators':[3, 10, 30], 'max_features':[2, 4, 6, 8]},\n",
    "    {'bootstrap':[False],'n_estimators':[3, 10],'max_features':[2,3,4]},\n",
    "]\n",
    "forest_reg=RandomForestRegressor()\n",
    "grid_search=GridSearchCV(forest_reg,param_grid,cv=5,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        return_train_score=True)\n",
    "grid_search.fit(housing_prepared,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres=grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"],cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score),params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances=grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs=[\"rooms_per_hhold\",\"pop_per_hhold\",\"bedrooms_per_room\"]\n",
    "cat_encoder=full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs=list(cat_encoder.categories_[0])\n",
    "attributes=num_attribs+extra_attribs+cat_one_hot_attribs\n",
    "sorted(zip(feature_importances,attributes),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating our System on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=grid_search.best_estimator_\n",
    "X_test=strat_test_set.drop(\"median_house_value\",axis=1)\n",
    "y_test=strat_test_set.median_house_value.copy()\n",
    "X_test_prepared=full_pipeline.transform(X_test)\n",
    "final_predictions=final_model.predict(X_test_prepared)\n",
    "final_mse=mean_squared_error(y_test,final_predictions)\n",
    "final_rmse=np.sqrt(final_mse)\n",
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "confidence=0.95\n",
    "squared_errors=(final_predictions-y_test)**2\n",
    "np.sqrt(stats.t.interval(confidence,len(squared_errors)-1,\n",
    "                        loc=squared_errors.mean(),\n",
    "                        scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
